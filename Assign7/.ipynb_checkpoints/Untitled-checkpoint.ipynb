{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9bc5d3-809b-4542-834c-3615320dc52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/TE/.local/lib/python3.10/site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/TE/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/TE/.local/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/TE/.local/lib/python3.10/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/TE/.local/lib/python3.10/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/TE/.local/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /home/TE/.local/lib/python3.10/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/TE/.local/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/TE/.local/lib/python3.10/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/TE/.local/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/TE/.local/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/TE/.local/lib/python3.10/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib64/python3.10/site-packages (from spacy) (1.24.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/TE/.local/lib/python3.10/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/TE/.local/lib/python3.10/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from spacy) (75.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/TE/.local/lib/python3.10/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/TE/.local/lib/python3.10/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/TE/.local/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/TE/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/TE/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib64/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/TE/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /home/TE/.local/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/TE/.local/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/TE/.local/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/TE/.local/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/TE/.local/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/TE/.local/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib64/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/TE/.local/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/TE/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib64/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/TE/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "132123e5-3b1e-40df-8124-d3ac65c9bdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: docx in /home/TE/.local/lib/python3.10/site-packages (0.2.4)\n",
      "Requirement already satisfied: python-docx in /home/TE/.local/lib/python3.10/site-packages (1.1.2)\n",
      "Requirement already satisfied: docx2txt in /home/TE/.local/lib/python3.10/site-packages (0.9)\n",
      "Requirement already satisfied: lxml in /usr/lib64/python3.10/site-packages (from docx) (4.7.1)\n",
      "Requirement already satisfied: Pillow>=2.0 in /usr/lib64/python3.10/site-packages (from docx) (9.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/site-packages (from python-docx) (4.12.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install docx python-docx docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed1328f-21f3-4afa-b187-3c4469982b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a8c3577-e165-4c29-95da-af3b2de2800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2e1829e-43a4-4673-991a-8135aad24734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/TE/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/TE/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/TE/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "document = \"\"\"Natural language processing (NLP) is a sub-field of artificial intelligence (AI) that focuses on the interaction between computers and human language.\n",
    "The ultimate goal of NLP is to enable computers to better understand, interpret, and generate human language.\"\"\"\n",
    "document_french = \"\"\"Le traitement du langage naturel (TALN) est un sous-domaine de l'intelligence artificielle (IA) qui se concentre sur l'interaction entre les ordinateurs et le langage humain.\n",
    "L'objectif ultime du TALN est de permettre aux ordinateurs de mieux comprendre, interpréter et générer le langage humain.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "443d1535-6998-4356-9b4f-c6c60c5d1104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['Le', 'traitement', 'du', 'langage', 'naturel', '(', 'TALN', ')', 'est', 'un', 'sous-domaine', 'de', \"l'intelligence\", 'artificielle', '(', 'IA', ')', 'qui', 'se', 'concentre', 'sur', \"l'interaction\", 'entre', 'les', 'ordinateurs', 'et', 'le', 'langage', 'humain', '.', \"L'objectif\", 'ultime', 'du', 'TALN', 'est', 'de', 'permettre', 'aux', 'ordinateurs', 'de', 'mieux', 'comprendre', ',', 'interpréter', 'et', 'générer', 'le', 'langage', 'humain', '.'] \n",
      "\n",
      "POS Tags: [('Natural', 'ADJ'), ('language', 'NOUN'), ('processing', 'NOUN'), ('(', 'PUNCT'), ('NLP', 'PROPN'), (')', 'PUNCT'), ('is', 'AUX'), ('a', 'DET'), ('sub', 'ADJ'), ('-', 'NOUN'), ('field', 'NOUN'), ('of', 'ADP'), ('artificial', 'ADJ'), ('intelligence', 'NOUN'), ('(', 'PUNCT'), ('AI', 'PROPN'), (')', 'PUNCT'), ('that', 'PRON'), ('focuses', 'VERB'), ('on', 'ADP'), ('the', 'DET'), ('interaction', 'NOUN'), ('between', 'ADP'), ('computers', 'NOUN'), ('and', 'CCONJ'), ('human', 'ADJ'), ('language', 'NOUN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('The', 'DET'), ('ultimate', 'ADJ'), ('goal', 'NOUN'), ('of', 'ADP'), ('NLP', 'PROPN'), ('is', 'AUX'), ('to', 'PART'), ('enable', 'VERB'), ('computers', 'NOUN'), ('to', 'PART'), ('better', 'ADV'), ('understand', 'VERB'), (',', 'PUNCT'), ('interpret', 'ADJ'), (',', 'PUNCT'), ('and', 'CCONJ'), ('generate', 'VERB'), ('human', 'ADJ'), ('language', 'NOUN'), ('.', 'PUNCT')] \n",
      "\n",
      "Filtered Tokens (Stop Words Removed): ['Le', 'traitement', 'du', 'langage', 'naturel', '(', 'TALN', ')', 'est', 'un', 'sous-domaine', 'de', \"l'intelligence\", 'artificielle', '(', 'IA', ')', 'qui', 'se', 'concentre', 'sur', \"l'interaction\", 'entre', 'les', 'ordinateurs', 'et', 'le', 'langage', 'humain', '.', \"L'objectif\", 'ultime', 'du', 'TALN', 'est', 'de', 'permettre', 'aux', 'ordinateurs', 'de', 'mieux', 'comprendre', ',', 'interpréter', 'et', 'générer', 'le', 'langage', 'humain', '.'] \n",
      "\n",
      "Stemmed Tokens: ['le', 'traitement', 'du', 'langag', 'naturel', '(', 'taln', ')', 'est', 'un', 'sous-domain', 'de', \"l'intellig\", 'artificiel', '(', 'ia', ')', 'qui', 'se', 'concentr', 'sur', \"l'interact\", 'entr', 'le', 'ordinateur', 'et', 'le', 'langag', 'humain', '.', \"l'objectif\", 'ultim', 'du', 'taln', 'est', 'de', 'permettr', 'aux', 'ordinateur', 'de', 'mieux', 'comprendr', ',', 'interprét', 'et', 'générer', 'le', 'langag', 'humain', '.'] \n",
      "\n",
      "Snow-Stemmed Tokens: ['le', 'trait', 'du', 'langag', 'naturel', '(', 'taln', ')', 'est', 'un', 'sous-domain', 'de', \"l'intelligent\", 'artificiel', '(', 'ia', ')', 'qui', 'se', 'concentr', 'sur', \"l'interact\", 'entre', 'le', 'ordin', 'et', 'le', 'langag', 'humain', '.', \"l'object\", 'ultim', 'du', 'taln', 'est', 'de', 'permettr', 'aux', 'ordin', 'de', 'mieux', 'comprendr', ',', 'interpret', 'et', 'géner', 'le', 'langag', 'humain', '.'] \n",
      "\n",
      "Lemmatized Tokens: ['Le', 'traitement', 'du', 'langage', 'naturel', '(', 'TALN', ')', 'est', 'un', 'sous-domaine', 'de', \"l'intelligence\", 'artificielle', '(', 'IA', ')', 'qui', 'se', 'concentre', 'sur', \"l'interaction\", 'entre', 'le', 'ordinateurs', 'et', 'le', 'langage', 'humain', '.', \"L'objectif\", 'ultime', 'du', 'TALN', 'est', 'de', 'permettre', 'aux', 'ordinateurs', 'de', 'mieux', 'comprendre', ',', 'interpréter', 'et', 'générer', 'le', 'langage', 'humain', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(document_french)\n",
    "\n",
    "doc = nlp(document)\n",
    "pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "snow_stemmer = SnowballStemmer(language='french')\n",
    "snow_stemmed_tokens = [snow_stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "print(f\"Original Tokens: {tokens} \\n\")\n",
    "print(f\"POS Tags: {pos_tags} \\n\")\n",
    "print(f\"Filtered Tokens (Stop Words Removed): {filtered_tokens} \\n\")\n",
    "print(f\"Stemmed Tokens: {stemmed_tokens} \\n\")\n",
    "print(f\"Snow-Stemmed Tokens: {snow_stemmed_tokens} \\n\")\n",
    "print(f\"Lemmatized Tokens: {lemmatized_tokens} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e7ee71d8-10e2-45cf-954d-326cc26ea3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"\"\"Natural language processing (NLP) is a sub-field of artificial intelligence (AI) that focuses on the interaction between computers and human language.\"\"\",\n",
    "    \"\"\"The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language.\"\"\",\n",
    "    \"\"\"AI and machine learning are transforming the way we live and work.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e56b996-1984-421b-8719-0f6398aa17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "terms = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "072ce597-b510-4726-a455-3a1896644d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms: ['ai' 'artificial' 'computers' 'enable' 'field' 'focuses' 'generate'\n",
      " 'goal' 'human' 'intelligence' 'interaction' 'interpret' 'language'\n",
      " 'learning' 'live' 'machine' 'natural' 'nlp' 'processing' 'sub'\n",
      " 'transforming' 'ultimate' 'understand' 'way' 'work']\n",
      "\n",
      "Document 1 TF-IDF values:\n",
      "ai: 0.2140\n",
      "artificial: 0.2814\n",
      "computers: 0.2140\n",
      "field: 0.2814\n",
      "focuses: 0.2814\n",
      "human: 0.2140\n",
      "intelligence: 0.2814\n",
      "interaction: 0.2814\n",
      "language: 0.4280\n",
      "natural: 0.2814\n",
      "nlp: 0.2140\n",
      "processing: 0.2814\n",
      "sub: 0.2814\n",
      "\n",
      "Document 2 TF-IDF values:\n",
      "computers: 0.2638\n",
      "enable: 0.3468\n",
      "generate: 0.3468\n",
      "goal: 0.3468\n",
      "human: 0.2638\n",
      "interpret: 0.3468\n",
      "language: 0.2638\n",
      "nlp: 0.2638\n",
      "ultimate: 0.3468\n",
      "understand: 0.3468\n",
      "\n",
      "Document 3 TF-IDF values:\n",
      "ai: 0.2965\n",
      "learning: 0.3899\n",
      "live: 0.3899\n",
      "machine: 0.3899\n",
      "transforming: 0.3899\n",
      "way: 0.3899\n",
      "work: 0.3899\n"
     ]
    }
   ],
   "source": [
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "print(\"Terms:\", terms)\n",
    "for doc_idx, doc in enumerate(tfidf_array):\n",
    "    print(f\"\\nDocument {doc_idx + 1} TF-IDF values:\")\n",
    "    for term_idx, value in enumerate(doc):\n",
    "        if value > 0: \n",
    "            print(f\"{terms[term_idx]}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
