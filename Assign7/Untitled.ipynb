{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd66f9e3-c36c-48e2-83dd-5852dd64eba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /Users/yashraj146/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/yashraj146/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/yashraj146/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/yashraj146/Library/Python/3.9/lib/python/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "\u001b[33m  WARNING: The script nltk is installed in '/Users/yashraj146/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nltk-3.9.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf700c3-d595-414f-a045-b64f3cadd81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a8c3577-e165-4c29-95da-af3b2de2800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1829e-43a4-4673-991a-8135aad24734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "document = \"\"\"Natural language processing (NLP) is a sub-field of artificial intelligence (AI) that focuses on the interaction between computers and human language.\n",
    "The ultimate goal of NLP is to enable computers to better understand, interpret, and generate human language.\"\"\"\n",
    "document_french = \"\"\"Le traitement du langage naturel (TALN) est un sous-domaine de l'intelligence artificielle (IA) qui se concentre sur l'interaction entre les ordinateurs et le langage humain.\n",
    "L'objectif ultime du TALN est de permettre aux ordinateurs de mieux comprendre, interpréter et générer le langage humain.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac3dd2-4e52-4965-b01c-5fdfefd0e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(document_french)\n",
    "\n",
    "doc = nlp(document)\n",
    "pos_tags = [(token.text, token.pos_) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fc0cc9-e34c-4944-bf19-34569a3b3604",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee7de16-8f25-4233-ae92-c6d3cf4b2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "snow_stemmer = SnowballStemmer(language='french')\n",
    "snow_stemmed_tokens = [snow_stemmer.stem(word) for word in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f538a-7647-43b4-91d0-05445a3e2d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac0477-d0a1-48a0-80b7-9098ca840620",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original Tokens: {tokens} \\n\")\n",
    "print(f\"POS Tags: {pos_tags} \\n\")\n",
    "print(f\"Filtered Tokens (Stop Words Removed): {filtered_tokens} \\n\")\n",
    "print(f\"Stemmed Tokens: {stemmed_tokens} \\n\")\n",
    "print(f\"Snow-Stemmed Tokens: {snow_stemmed_tokens} \\n\")\n",
    "print(f\"Lemmatized Tokens: {lemmatized_tokens} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ee71d8-10e2-45cf-954d-326cc26ea3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"\"\"Natural language processing (NLP) is a sub-field of artificial intelligence (AI) that focuses on the interaction between computers and human language.\"\"\",\n",
    "    \"\"\"The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language.\"\"\",\n",
    "    \"\"\"AI and machine learning are transforming the way we live and work.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e56b996-1984-421b-8719-0f6398aa17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "terms = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "072ce597-b510-4726-a455-3a1896644d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms: ['ai' 'artificial' 'computers' 'enable' 'field' 'focuses' 'generate'\n",
      " 'goal' 'human' 'intelligence' 'interaction' 'interpret' 'language'\n",
      " 'learning' 'live' 'machine' 'natural' 'nlp' 'processing' 'sub'\n",
      " 'transforming' 'ultimate' 'understand' 'way' 'work']\n",
      "\n",
      "Document 1 TF-IDF values:\n",
      "ai: 0.2140\n",
      "artificial: 0.2814\n",
      "computers: 0.2140\n",
      "field: 0.2814\n",
      "focuses: 0.2814\n",
      "human: 0.2140\n",
      "intelligence: 0.2814\n",
      "interaction: 0.2814\n",
      "language: 0.4280\n",
      "natural: 0.2814\n",
      "nlp: 0.2140\n",
      "processing: 0.2814\n",
      "sub: 0.2814\n",
      "\n",
      "Document 2 TF-IDF values:\n",
      "computers: 0.2638\n",
      "enable: 0.3468\n",
      "generate: 0.3468\n",
      "goal: 0.3468\n",
      "human: 0.2638\n",
      "interpret: 0.3468\n",
      "language: 0.2638\n",
      "nlp: 0.2638\n",
      "ultimate: 0.3468\n",
      "understand: 0.3468\n",
      "\n",
      "Document 3 TF-IDF values:\n",
      "ai: 0.2965\n",
      "learning: 0.3899\n",
      "live: 0.3899\n",
      "machine: 0.3899\n",
      "transforming: 0.3899\n",
      "way: 0.3899\n",
      "work: 0.3899\n"
     ]
    }
   ],
   "source": [
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "print(\"Terms:\", terms)\n",
    "for doc_idx, doc in enumerate(tfidf_array):\n",
    "    print(f\"\\nDocument {doc_idx + 1} TF-IDF values:\")\n",
    "    for term_idx, value in enumerate(doc):\n",
    "        if value > 0: \n",
    "            print(f\"{terms[term_idx]}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca9aa2-a4f1-4a2e-a32e-b0636dc64522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
